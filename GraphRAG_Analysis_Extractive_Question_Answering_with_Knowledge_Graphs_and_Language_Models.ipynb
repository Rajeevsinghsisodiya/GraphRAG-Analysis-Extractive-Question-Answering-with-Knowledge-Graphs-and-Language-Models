{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project name: 'GraphRAG Analysis: Extractive Question Answering with Knowledge Graphs and Language Models'\n",
        "\n",
        "Contributor: Rajeev singh sisodiya\n",
        "\n",
        "Project overview:\n",
        "This project explores using a combination of knowledge graphs and large language models (LLMs) for extractive question answering. The system leverages a PDF document as input and aims to answer user questions based on the information within it.\n",
        "\n",
        "Information Extraction:\n",
        "Processes a PDF document using a PyPDFLoader.\n",
        "Splits the document content into smaller chunks using a RecursiveCharacterTextSplitter.\n",
        "\n",
        "Knowledge Graph Creation:\n",
        "Utilizes an LLM (ChatOpenAI) to identify key entities and relationships within the document text.\n",
        "Creates a knowledge graph in Neo4j based on the extracted entities and relationships.\n",
        "Integrates FAISS (vector search) for document retrieval.\n",
        "\n",
        "#Question Answering\n",
        "\n",
        "Offers two retrieval methods:\n",
        "FAISS retriever: Efficiently retrieves relevant document passages based on the question using vector similarity search.\n",
        "\n",
        "Cypher-based Neo4j retriever:\n",
        "Retrieves entities from the knowledge graph that match the user's query.\n",
        "Employs another LLM (ChatOpenAI) to generate concise answers to the user's questions based on the retrieved information.\n",
        "\n",
        "Evaluation:\n",
        "Generates ground truth question-answer pairs for the processed document.\n",
        "Evaluates the performance of the two retrieval methods (FAISS and Neo4j) using RAG (Reasoning Augumented Generation) metrics like faithfulness, answer relevancy, context relevancy, and context recall.\n",
        "\n",
        "This project demonstrates the potential of combining knowledge graphs and LLMs to build robust and informative question answering systems for textual data."
      ],
      "metadata": {
        "id": "ZhQqsaI6AH7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting Up the Environment\n",
        "First, let's set up our environment and import the necessary libraries:"
      ],
      "metadata": {
        "id": "Czae7gs5AlO0"
      }
    },
    {
      "source": [
        "!pip install python-dotenv"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMnCAKfMeHr9",
        "outputId": "0e3c6cc4-59d1-4c94-a5a3-42bf83a7189e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install langchain_openai"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s07RhIEGeS6Y",
        "outputId": "32e29bf4-7fc8-4a07-e0b7-e41003018790"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.23-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.35 (from langchain_openai)\n",
            "  Downloading langchain_core-0.2.38-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n",
            "  Downloading openai-1.43.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.35->langchain_openai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3.0,>=0.2.35->langchain_openai)\n",
            "  Downloading langsmith-0.1.114-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (2.8.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.3.0,>=0.2.35->langchain_openai)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain_openai)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.35->langchain_openai)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.35->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.35->langchain_openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Downloading langchain_openai-0.1.23-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.38-py3-none-any.whl (396 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.43.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.114-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.0/290.0 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, jiter, h11, tiktoken, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langchain_openai\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.38 langchain_openai-0.1.23 langsmith-0.1.114 openai-1.43.0 orjson-3.10.7 tenacity-8.5.0 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install langchain-community"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBhmtBsyefaa",
        "outputId": "84cf9f17-04ba-4705-fb84-5e2a8f3603da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.16 (from langchain-community)\n",
            "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.38)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.114)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.16->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.2.16-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-0.2.16 langchain-community-0.2.16 langchain-text-splitters-0.2.4 marshmallow-3.22.0 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install neo4j"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_CwaqrGfwXw",
        "outputId": "c2150e12-850b-4a09-9d82-8996c6ed86e8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neo4j\n",
            "  Downloading neo4j-5.24.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2024.1)\n",
            "Downloading neo4j-5.24.0-py3-none-any.whl (294 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/294.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neo4j\n",
            "Successfully installed neo4j-5.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade ragas # Upgrade to the latest version of ragas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FRQHZXeqgZ2y",
        "outputId": "205ea65b-db5b-473d-b992-d15f01df6a6c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ragas in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ragas) (1.26.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from ragas) (2.21.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from ragas) (0.7.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from ragas) (0.2.16)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (from ragas) (0.2.38)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (from ragas) (0.2.16)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (from ragas) (0.1.23)\n",
            "Requirement already satisfied: openai>1 in /usr/local/lib/python3.10/dist-packages (from ragas) (1.43.0)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from ragas) (0.3.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ragas) (1.4.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>1->ragas) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (2.0.32)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (0.1.114)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core->ragas) (1.33)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas) (0.6.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas) (2024.5.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (1.2.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>1->ragas) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>1->ragas) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->ragas) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->ragas) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ragas.metrics\n",
        "print(dir(ragas.metrics))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qjrrEhJhlgt",
        "outputId": "2a898e0a-42dd-4cc6-dd97-85c61cf84015"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AnswerCorrectness', 'AnswerRelevancy', 'AnswerSimilarity', 'AspectCritique', 'ContextEntityRecall', 'ContextPrecision', 'ContextRecall', 'ContextUtilization', 'Faithfulness', 'FaithulnesswithHHEM', 'LabelledRubricsScore', 'NoiseSensitivity', 'ReferenceFreeRubricsScore', 'SummarizationScore', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_answer_correctness', '_answer_relevance', '_answer_similarity', '_context_entities_recall', '_context_precision', '_context_recall', '_faithfulness', '_noise_sensitivity', '_rubrics_based', '_summarization', 'answer_correctness', 'answer_relevancy', 'answer_similarity', 'base', 'context_entity_recall', 'context_precision', 'context_recall', 'context_utilization', 'critique', 'faithfulness', 'labelled_rubrics_score', 'noise_sensitivity_irrelevant', 'noise_sensitivity_relevant', 'reference_free_rubrics_score', 'summarization_score']\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from typing import List, Dict, Union\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Neo4jVector, FAISS\n",
        "\n",
        "import langchain\n",
        "from langchain.schema import BaseRetriever\n",
        "\n",
        "# Remove the duplicate import of RunnablePassthrough\n",
        "#from langchain.runnables import RunnablePassthrough\n",
        "from langchain_core.documents import Document\n",
        "from langchain.schema import OutputParserException\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough # This import is sufficient\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "from ragas import evaluate\n",
        "\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_recall\n",
        "\n",
        "\n",
        "from datasets import Dataset\n",
        "import random\n",
        "\n",
        "import re\n",
        "from tqdm.asyncio import tqdm\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Load environment variables for API keys\n",
        "load_dotenv()\n",
        "\n",
        "openai_api_key = os.getenv(\"sk-UUXg6tJgz4oVkt9Sx4lWwWSS7fbPdqrDbAtAu0YsReT3BlbkFJzMEGGwT2caQXxodaahMvf6kmaN0CA2Jsvzs-BA0eEA\")\n",
        "neo4j_url = os.getenv(\"neo4j+s://db14414e.databases.neo4j.io\")\n",
        "\n",
        "neo4j_user = os.getenv(\"neo4j\")\n",
        "neo4j_password = os.getenv(\"1uwMBy5pleVc8rmrOeunHEIHaUXz89_9rWHEgaISYXM\")\n",
        "\n",
        "# nest_asyncio applies a fix for asyncio running in Jupyter notebooks\n",
        "nest_asyncio.apply()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8ExDHJq3jVF1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting Up Neo4j Connection\n",
        "To use Neo4j as the graph database, let's set up the connection and create some utility functions"
      ],
      "metadata": {
        "id": "xEnsiArPBeua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Verify Environment Variables"
      ],
      "metadata": {
        "id": "kpg3G7m-u7Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Fetch environment variables\n",
        "# Replace 'neo4j+s' with 'bolt+s' if necessary\n",
        "neo4j_url = os.getenv(\"neo4j+s://db14414e.databases.neo4j.io\")\n",
        "neo4j_user = os.getenv(\"neo4j\")\n",
        "neo4j_password = os.getenv(\"1uwMBy5pleVc8rmrOeunHEIHaUXz89_9rWHEgaISYXM\")\n",
        "\n",
        "# Print values to verify\n",
        "if not neo4j_url or not neo4j_user or not neo4j_password:\n",
        "    print(\"Error: One or more environment variables are missing or incorrect\")\n",
        "    print(f\"Neo4j URL: {neo4j_url}\")\n",
        "    print(f\"Neo4j User: {neo4j_user}\")\n",
        "    print(f\"Neo4j Password: {neo4j_password}\")\n",
        "else:\n",
        "    print(\"All environment variables are loaded correctly.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X4gRU3aoM16",
        "outputId": "c9444277-93f9-457a-8f4e-5d4cb0ba7e36"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: One or more environment variables are missing or incorrect\n",
            "Neo4j URL: None\n",
            "Neo4j User: None\n",
            "Neo4j Password: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Explicitly specify the path to your .env file if it's not in the same directory\n",
        "load_dotenv('.env')\n",
        "\n",
        "# Retrieve OpenAI API key and Neo4j credentials from the environment variables\n",
        "#please use your personal openai API key and neo4j credentials. I uesd here my personal credentials.\n",
        "openai_api_key = os.getenv(\"sk-UUXg6tJgz4oVkt9Sx4lWwWSS7fbPdqrDbAtAu0YsReT3BlbkFJzMEGGwT2caQXxodaahMvf6kmaN0CA2Jsvzs-BA0eEA\")  # Correct the environment variable name if needed\n",
        "neo4j_url = os.getenv(\"neo4j+s://db14414e.databases.neo4j.io\")\n",
        "neo4j_user = os.getenv(\"neo4j\")\n",
        "neo4j_password = os.getenv(\"1uwMBy5pleVc8rmrOeunHEIHaUXz89_9rWHEgaISYXM\")\n",
        "\n",
        "# Print values to verify they are loaded correctly\n",
        "print(\"Neo4j URL:\", neo4j_url)\n",
        "print(\"Neo4j User:\", neo4j_user)\n",
        "print(\"Neo4j Password:\", neo4j_password)\n",
        "\n",
        "# Adjust the URI scheme to 'bolt+s' for Neo4j Aura if needed\n",
        "if neo4j_url and neo4j_url.startswith(\"neo4j+s://\"):\n",
        "    neo4j_url = neo4j_url.replace(\"neo4j+s://\", \"bolt+s://\")\n",
        "    print(\"Corrected Neo4j URL:\", neo4j_url)\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Create Neo4j driver instance and establish connection\n",
        "try:\n",
        "    driver = GraphDatabase.driver(neo4j_url, auth=(neo4j_user, neo4j_password))\n",
        "    print(\"Neo4j driver created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to Neo4j: {e}\")\n",
        "\n",
        "# Function to clear the Neo4j instance\n",
        "def clear_neo4j_data(tx):\n",
        "    tx.run(\"MATCH (n) DETACH DELETE n\")\n",
        "\n",
        "# Ensure vector index exists in Neo4j\n",
        "def ensure_vector_index(recreate=False):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"\"\"\n",
        "            SHOW INDEXES\n",
        "            YIELD name, labelsOrTypes, properties\n",
        "            WHERE name = 'entity_index'\n",
        "            AND labelsOrTypes = ['Entity']\n",
        "            AND properties = ['embedding']\n",
        "            RETURN count(*) > 0 AS exists\n",
        "        \"\"\").single()\n",
        "\n",
        "        index_exists = result['exists'] if result else False\n",
        "\n",
        "        if index_exists and recreate:\n",
        "            session.run(\"DROP INDEX entity_index\")\n",
        "            print(\"Existing vector index 'entity_index' dropped.\")\n",
        "            index_exists = False\n",
        "\n",
        "        if not index_exists:\n",
        "            session.run(\"\"\"\n",
        "                CALL db.index.vector.createNodeIndex(\n",
        "                    'entity_index',\n",
        "                    'Entity',\n",
        "                    'embedding',\n",
        "                    1536,\n",
        "                    'cosine'\n",
        "                )\n",
        "            \"\"\")\n",
        "            print(\"Vector index 'entity_index' created successfully.\")\n",
        "        else:\n",
        "            print(\"Vector index 'entity_index' already exists. Skipping creation.\")\n",
        "\n",
        "# Add embeddings to entities in Neo4j\n",
        "def add_embeddings_to_entities(tx, embeddings):\n",
        "    query = \"\"\"\n",
        "        MATCH (e:Entity)\n",
        "        WHERE e.embedding IS NULL\n",
        "        WITH e LIMIT 100\n",
        "        SET e.embedding = $embedding\n",
        "    \"\"\"\n",
        "\n",
        "    entities = tx.run(\"MATCH (e:Entity) WHERE e.embedding IS NULL RETURN e.name AS name LIMIT 100\").data()\n",
        "\n",
        "    for entity in tqdm(entities, desc=\"Adding embeddings\"):\n",
        "        embedding = embeddings.embed_query(entity['name'])\n",
        "        tx.run(query, embedding=embedding)\n",
        "\n",
        "# Example of using the functions\n",
        "with driver.session() as session:\n",
        "    session.write_transaction(clear_neo4j_data)\n",
        "    ensure_vector_index()\n",
        "    # Assuming 'embeddings' is an object you have for embedding purposes\n",
        "    # session.write_transaction(add_embeddings_to_entities, embeddings)\n"
      ],
      "metadata": {
        "id": "CrZmbkWCBhfP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing and Graph Creation\n",
        "Now, let's load our data and create our knowledge graph"
      ],
      "metadata": {
        "id": "Iux9taL-B9LD"
      }
    },
    {
      "source": [
        "!pip install pypdf"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8cZX6TXlx2Jv",
        "outputId": "c3d40afc-b1e5-4915-a920-cfb0c15798b8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/295.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pdf link chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.astrid-online.it/static/upload/tran/transcript-of-the-second-presidential-debate.pdf\n",
        "!pip install pypdf\n",
        "# Load and process the PDF\n",
        "pdf_path = \"/content/transcript-of-the-second-presidential-debate.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Function to create graph structure\n",
        "def create_graph_structure(tx, texts):\n",
        "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        Given the following text, identify key entities and their relationships.\n",
        "\n",
        "        Format the output as a list of tuples, each on a new line: (entity1, relationship, entity2)\n",
        "\n",
        "        Text: {text}\n",
        "\n",
        "        Entities and Relationships:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    for text in tqdm(texts, desc=\"Creating graph structure\"):\n",
        "        response = llm(prompt.format_messages(text=text.page_content))\n",
        "\n",
        "        # Process the response and create nodes and relationships\n",
        "        lines = response.content.strip().split('\\n')\n",
        "\n",
        "        for line in lines:\n",
        "            if line.startswith('(') and line.endswith(')'):\n",
        "                parts = line[1:-1].split(',')\n",
        "                if len(parts) == 3:\n",
        "                    entity1, relationship, entity2 = [part.strip() for part in parts]\n",
        "\n",
        "                    # Create nodes and relationship\n",
        "                    query = (\n",
        "                        \"\"\"\n",
        "                        MERGE (e1:Entity {name: $entity1})\n",
        "                        MERGE (e2:Entity {name: $entity2})\n",
        "                        MERGE (e1)-[:RELATED (type: $relationship)]->(e2)\n",
        "                        \"\"\"\n",
        "                    )\n",
        "                    tx.run(query, entity1=entity1, entity2=entity2, relationship=relationship)"
      ],
      "metadata": {
        "id": "NmeYn_GkCS_m"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setting Up Retrievers\n",
        "We'll set up two types of retrievers: one using FAISS for vector-based retrieval, and another using Neo4j for graph-based retrieval."
      ],
      "metadata": {
        "id": "9PA8ROqECUNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings model\n",
        "embeddings = OpenAIEmbeddings(openai_api_key='sk-UUXg6tJgz4oVkt9Sx4lWwWSS7fbPdqrDbAtAu0YsReT3BlbkFJzMEGGwT2caQXxodaahMvf6kmaN0CA2Jsvzs-BA0eEA')\n",
        "\n",
        "embedding_cache = {}\n",
        "\n",
        "def get_embedding(text):\n",
        "        if text not in embedding_cache:\n",
        "            embedding_cache[text] = embeddings.embed_query(text)  # Assuming you're using embed_query for single texts\n",
        "        return embedding_cache[text]\n",
        "        embeddings.embed_query = get_embedding\n",
        "\n",
        "# Create FAISS retriever\n",
        "faiss_vector_store = FAISS.from_documents(texts, embeddings)\n",
        "faiss_retriever = faiss_vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# Neo4j retriever\n",
        "def create_neo4j_retriever():\n",
        "    with driver.session() as session:\n",
        "        # Clear existing data\n",
        "        session.run(\"MATCH (n) DETACH DELETE n\")  # equivalent to the clear_neo4j_data function created earlier in code\n",
        "\n",
        "        # Create graph structure\n",
        "        session.execute_write(create_graph_structure, texts)\n",
        "\n",
        "        # Add embeddings to entities\n",
        "        max_attempts = 10\n",
        "        attempt = 1\n",
        "\n",
        "        while attempt <= max_attempts:\n",
        "            count = session.execute_read(lambda tx: tx.run(\"MATCH (e:Entity) WHERE e.embedding IS NULL RETURN COUNT(e) AS count\").single()['count'])\n",
        "            if count == 0:\n",
        "                break\n",
        "\n",
        "            session.execute_write(add_embeddings_to_entities, embeddings)\n",
        "\n",
        "            if attempt == max_attempts:\n",
        "                print(\"Warning: Not all entities have embeddings after maximum attempts.\")\n",
        "\n",
        "        # Create Neo4j retriever\n",
        "        neo4j_vector_store = Neo4jVector.from_existing_index(\n",
        "            embeddings,\n",
        "            url=neo4j_url,\n",
        "            username=neo4j_user,\n",
        "            password=neo4j_password,\n",
        "            index_name=\"entity_index\",\n",
        "            node_label=\"Entity\",\n",
        "            text_node_property=\"name\",\n",
        "            embedding_node_property=\"embedding\"\n",
        "        )\n",
        "\n",
        "        return neo4j_vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# Cypher-based retriever\n",
        "def cypher_retriever(search_term: str) -> List[Document]:\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"\"\"\n",
        "            MATCH (e:Entity)\n",
        "            WHERE e.name CONTAINS $search_term\n",
        "            RETURN e.name AS name, [(e)-[r:RELATED]->(related) | related.name, r.type] AS related\n",
        "            LIMIT 2\n",
        "        \"\"\", search_term=search_term)\n",
        "\n",
        "        documents = []\n",
        "        for record in result:\n",
        "            content = f\"Entity: {record['name']}\\nRelated: {', '.join(record['related'])}\"\n",
        "            documents.append(Document(page_content=content))\n",
        "\n",
        "        return documents"
      ],
      "metadata": {
        "id": "NBzeN0fVCaGK"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating RAG Chains\n",
        "Now, let's create our RAG chains"
      ],
      "metadata": {
        "id": "0YnVqF8EDFTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rag_chain(retriever):\n",
        "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "    template = \"\"\"Answer the question based on the following context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "    if callable(retriever):\n",
        "        # For Cypher retriever\n",
        "        retriever_func = lambda q: retriever(q)\n",
        "    else:\n",
        "        # For FAISS retriever\n",
        "        retriever_func = retriever\n",
        "\n",
        "    return (\n",
        "        {\"context\": retriever_func, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | StroutputParser()\n",
        "    )\n",
        "\n",
        "# Embeddings model\n",
        "embeddings = OpenAIEmbeddings(openai_api_key='sk-UUXg6tJgz4oVkt9Sx4lWwWSS7fbPdqrDbAtAu0YsReT3BlbkFJzMEGGwT2caQXxodaahMvf6kmaN0CA2Jsvzs-BA0eEA')\n",
        "# Create FAISS retriever\n",
        "faiss_vector_store = FAISS.from_documents(texts, embeddings)\n",
        "faiss_retriever = faiss_vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# Create RAG chains\n",
        "faiss_rag_chain = create_rag_chain(faiss_retriever)\n",
        "cypher_rag_chain = create_rag_chain(cypher_retriever)"
      ],
      "metadata": {
        "id": "gze4WscCDHYI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Setup\n",
        "To evaluate our RAG systems, we'll create a ground truth dataset and use the RAGAS framework"
      ],
      "metadata": {
        "id": "Hovygo3IDfuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ground_truth(texts: List[Union[str, Document]], num_questions: int = 100) -> List[Dict]:\n",
        "    llm_ground_truth = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.2)\n",
        "\n",
        "    def get_text(item):\n",
        "        return item.page_content if isinstance(item, Document) else item\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    all_splits = text_splitter.split_text(' '.join(get_text(doc) for doc in texts))\n",
        "\n",
        "    ground_truth = []\n",
        "\n",
        "    question_prompt = ChatPromptTemplate.from_template(\n",
        "        \"Given the following text, generate {num_questions} diverse and specific questions that can be answered based on the information in the text. \"\n",
        "        \"Provide the questions as a numbered list.\\n\\nText: {text}\\n\\nQuestions:\"\n",
        "    )\n",
        "\n",
        "    all_questions = []\n",
        "    for split in tqdm(all_splits, desc=\"Generating questions\"):\n",
        "        response = llm_ground_truth(question_prompt.format_messages(num_questions=3, text=split))\n",
        "        questions = response.content.strip().split('\\n')\n",
        "        all_questions.extend([q.split('. ', 1)[1] if '. ' in q else q for q in questions])\n",
        "\n",
        "    random.shuffle(all_questions)\n",
        "    selected_questions = all_questions[:num_questions]\n",
        "\n",
        "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "    for question in tqdm(selected_questions, desc=\"Generating ground truth\"):\n",
        "        answer_prompt = ChatPromptTemplate.from_template(\n",
        "            \"Given the following question, provide a concise and accurate answer based on the information available. \"\n",
        "            \"If the answer is not directly available, respond with 'Information not available in the given context.'\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "        )\n",
        "        answer_response = llm(answer_prompt.format_messages(question=question))\n",
        "        answer = answer_response.content.strip()\n",
        "\n",
        "        context_prompt = ChatPromptTemplate.from_template(\n",
        "            \"Given the following question and answer, provide a brief, relevant context that supports this answer. \"\n",
        "            \"If no relevant context is available, respond with 'No relevant context available.'\\n\\n\"\n",
        "            \"Question: {question}\\nAnswer: {answer}\\n\\nRelevant context:\"\n",
        "        )\n",
        "        context_response = llm(context_prompt.format_messages(question=question, answer=answer))\n",
        "        context = context_response.content.strip()\n",
        "\n",
        "        ground_truth.append({\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"context\": context,\n",
        "        })\n",
        "\n",
        "    return ground_truth\n",
        "\n",
        "async def evaluate_rag_async(rag_chain, ground_truth, name):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "    generated_answers = []\n",
        "    for item in tqdm(ground_truth, desc=f\"Evaluating {name}\"):\n",
        "        question = splitter.split_text(item[\"question\"])[0]\n",
        "\n",
        "        try:\n",
        "            answer = await rag_chain.ainvoke(question)\n",
        "        except AttributeError:\n",
        "            answer = rag_chain.invoke(question)\n",
        "\n",
        "        truncated_answer = splitter.split_text(str(answer))[0]\n",
        "        truncated_context = splitter.split_text(item[\"context\"])[0]\n",
        "        truncated_ground_truth = splitter.split_text(item[\"answer\"])[0]\n",
        "\n",
        "        generated_answers.append({\n",
        "            \"question\": question,\n",
        "            \"answer\": truncated_answer,\n",
        "            \"contexts\": [truncated_context],\n",
        "            \"ground_truth\": truncated_ground_truth\n",
        "        })\n",
        "\n",
        "    dataset = Dataset.from_pandas(pd.DataFrame(generated_answers))\n",
        "\n",
        "    result = evaluate(\n",
        "        dataset,\n",
        "        metrics=[\n",
        "            context_relevancy,\n",
        "            faithfulness,\n",
        "            answer_relevancy,\n",
        "            context_recall,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return {name: result}\n",
        "async def run_evaluations(rag_chains, ground_truth):\n",
        "    results = {}\n",
        "    for name, chain in rag_chains.items():\n",
        "        result = await evaluate_rag_async(chain, ground_truth, name)\n",
        "        results.update(result)\n",
        "    return results\n",
        "\n",
        "# Main execution function\n",
        "async def main():\n",
        "    # Ensure vector index\n",
        "    ensure_vector_index(recreate=True)\n",
        "\n",
        "    # Create retrievers\n",
        "    neo4j_retriever = create_neo4j_retriever()\n",
        "\n",
        "    # Create RAG chains\n",
        "    faiss_rag_chain = create_rag_chain(faiss_retriever)\n",
        "    neo4j_rag_chain = create_rag_chain(neo4j_retriever)\n",
        "\n",
        "    # Generate ground truth\n",
        "    ground_truth = create_ground_truth(texts)\n",
        "\n",
        "    # Run evaluations\n",
        "    rag_chains = {\n",
        "        \"FAISS\": faiss_rag_chain,\n",
        "        \"Neo4j\": neo4j_rag_chain\n",
        "    }\n",
        "    results = await run_evaluations(rag_chains, ground_truth)\n",
        "    return results\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()\n",
        "    try:\n",
        "        results = asyncio.run(asyncio.wait_for(main(), timeout=7200))  # 2 hour timeout\n",
        "        plot_results(results)\n",
        "\n",
        "        # Print detailed results\n",
        "        for name, result in results.items():\n",
        "            print(f\"Results for {name}:\")\n",
        "            print(result)\n",
        "            print()\n",
        "    except asyncio.TimeoutError:\n",
        "        print(\"Evaluation timed out after 2 hours.\")\n",
        "    finally:\n",
        "        # Close the Neo4j driver\n",
        "        driver.close()"
      ],
      "metadata": {
        "id": "9nneyj6aEHZp"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results and Analysis of the project:\n",
        "This project implements a system for extractive question answering using a combination of retrievers and large language models (LLMs) on a PDF document.\n",
        "\n",
        "1.Functionalities\n",
        "\n",
        "Information Extraction:\n",
        "\n",
        "Successfully extracts text chunks from the PDF document using PyPDFLoader and RecursiveCharacterTextSplitter.\n",
        "\n",
        "Knowledge Graph Creation:\n",
        "\n",
        "Utilizes ChatOpenAI to identify key entities and relationships from the extracted text.\n",
        "Creates a knowledge graph in Neo4j based on the identified entities and relationships.\n",
        "Integrates FAISS for efficient document retrieval based on vector similarity.\n",
        "\n",
        "2 .Question Answering:\n",
        "\n",
        "Offers two retrieval methods:\n",
        "\n",
        "FAISS retriever: Efficiently retrieves relevant document passages based on the question using vector similarity search.\n",
        "\n",
        "Cypher-based Neo4j retriever:\n",
        "Retrieves entities from the knowledge graph that match the user's query.\n",
        "Employs another LLM (ChatOpenAI) to generate concise answers to the user's questions based on the retrieved information.\n",
        "\n",
        "Evaluation:\n",
        "\n",
        "Generates ground truth question-answer pairs for the processed document.\n",
        "Evaluates the performance of the two retrieval methods (FAISS and Neo4j) using RAG (Reasoning Augmented Generation) metrics like faithfulness, answer relevancy, context relevancy, and context recall.\n",
        "\n",
        "3 .Expected Outcome:\n",
        "\n",
        "The project is designed to evaluate the effectiveness of FAISS and Neo4j retrievers in retrieving relevant information for question answering. The RAG metrics would ideally show:\n",
        "\n",
        "High faithfulness: The generated answers accurately reflect the retrieved information.\n",
        "\n",
        "High answer relevancy: The generated answers directly address the user's question.\n",
        "\n",
        "High context relevancy: The retrieved context snippets are relevant to the question and answer.\n",
        "\n",
        "High context recall: The retrieved context captures most of the important information needed for answering the question.\n",
        "\n",
        "#Analysis of Missing Information\n",
        "\n",
        "Unfortunately, the project doesn't include the plot_results function or the final printed results. However, based on the functionalities, we can infer that the analysis would involve examining the RAG metric scores for both FAISS and Neo4j retrievers.\n",
        "\n",
        "Here are some pointers for further analysis:\n",
        "\n",
        "Compare the RAG metric scores between FAISS and Neo4j retrievers. Identify which retrieval method performs better in terms of faithfulness, answer relevancy, context relevancy, and context recall.\n",
        "\n",
        "Analyze potential reasons for the observed performance. Consider factors like the complexity of the knowledge graph, the effectiveness of entity and relationship identification by ChatOpenAI, and the quality of retrieved passages by FAISS or Neo4j.\n",
        "\n",
        "Overall, the project demonstrates a promising approach for extractive question answering using LLMs and knowledge graphs. Analyzing the RAG metrics would provide valuable insights into the effectiveness of different retrieval methods and guide further improvements to the system.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JEPPoS2BHuLQ"
      }
    }
  ]
}